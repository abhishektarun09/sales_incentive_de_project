# ðŸ›’ Grocery Chain Sales Data Engineering Pipeline

## Overview

This project is a data engineering pipeline designed to process sales data for a chain of grocery stores. The primary goal is to ingest raw sales data, enrich it with dimensional data, and produce valuable, aggregated data marts for business analysis.

The pipeline automates the following key business reports:
1.  **Customer Expenditure Mart**: Identifies monthly spending patterns for each customer, enabling targeted marketing campaigns (e.g., offering coupons to low-spending customers).
2.  **Salesperson Performance Mart**: Determines the top-performing salesperson for each store every month and calculates their performance-based incentive.

The pipeline is designed to be robust, handling schema variations in input files and isolating erroneous data for later inspection.

---

## Data Architecture & Workflow

The pipeline follows an Extract, Transform, Load (ETL) process:

1.  **Extract**:
    * Raw, transactional sales data is pulled from CSV files located in an AWS S3 bucket.
    * Enrichment data (customer, store, product, and sales team details) is fetched from a MySQL database.
2.  **Transform**:
    * The raw data is validated and cleaned within a PySpark DataFrame.
    * It is joined with the dimensional data to create a comprehensive dataset.
    * Two specific data marts are created through aggregations and window functions in Spark.
3.  **Load**:
    * The final, transformed data marts are written back to a different S3 location in the efficient **Parquet format**.
    * Any files that fail initial validation (e.g., not a CSV, incorrect format) are moved to an error directory in S3.



<p align="center">
  <img src="docs\architecture.png" alt="Architecture" width="400"/>
</p>

A simplified flow looks like this:
`AWS S3 (Raw Sales CSVs)` + `MySQL (Dimension Tables)` â†’ **PySpark ETL Process** â†’ `AWS S3 (Partitioned Parquet Data Marts & Error CSVs)`

---

## Data Sources

### Primary Data Source (Facts)
* **Source**: AWS S3
* **Path**: `s3://de-project-bucket/sales_data/`
* **Format**: CSV, JSON
* **Description**: Contains daily transactional sales records from various stores.

### Secondary Data Sources (Dimensions)
* **Source**: MySQL Database
* **Tables**:
    * `customer_table`: Details about each customer.
    * `store_table`: Details about each store location.
    * `sales_team_table`: Information on all sales personnel.
    * `product_table`: Details about each product.

---

## ETL Process Logic

### 1. Extraction & Validation
* The pipeline scans the `sales_data/` S3 prefix for new files.
* Each file is initially checked to ensure it is a valid CSV file.
* Files that are not valid CSVs or fail a basic structure check are immediately moved to `s3://de-project-bucket/error_files/` without further processing.

### 2. Transformation
* **Schema Handling**: The pipeline expects a set of mandatory columns ( `customer_id`, `store_id`, `product_name`, `sales_date`, `sales_person_id`, `price`, `quantity`, `total_cost` ). Any additional, non-mandatory columns found in the source files are consolidated into a single string column named `additional_columns`. This prevents the pipeline from failing due to unexpected schema changes.
* **Data Union**: All valid CSV files are read and unioned into a single PySpark DataFrame.
* **Enrichment**: The unified sales data is joined with the dimensional tables (`customer`, `store`, `sales_team`, `product`) from MySQL using their respective foreign keys.
<p align="center">
  <img src="docs\database_schema.drawio.png" alt="Architecture" width="400"/>
</p>

* **Data Mart Creation**:
    * **Customer Data Mart**:
        1.  The `sales_date` is used to derive `sales_date_month`.
        2.  The data is grouped by `customer_id` and `sales_date_month`.
        3.  The `total_cost` is summed up for each group to calculate `total_monthly_expenditure`.
    * **Sales Person Data Mart**:
        1.  The data is first grouped by `store_id`, `sales_person_id` and `sales_date_month`. The `total_cost` is summed to get the total revenue generated by each person in a given store for that month.
        2.  A **window function** `RANK()` is applied, partitioned by `store_id` and `sales_date_month`.
        3.  The top performer (where rank = 1) is selected for each partition.
        4.  An `incentive_amount` column is calculated for the top performer as `total_revenue * 0.01`. For all other salespersons, this value is 0.

### 3. Loading
* The two generated data marts are written to AWS S3 in **Parquet** format. They are **partitioned by year and month** to optimize query performance and reduce data scanning costs for downstream analytical tools.
* **Output Path for Customer Mart**: `s3://de-project-bucket/customer_data_mart/`
* **Output Path for Sales Person Mart**: `s3://de-project-bucket/salesperson_data_mart/`

---

## Output Data Marts Schema

### 1. Customer Monthly Expenditure
This data mart provides a monthly summary of spending per customer.
* **Partitioned by**: `customer_id`, `sales_date_month`

| Column Name                 | Data Type | Description                                        |
| --------------------------- | --------- | -------------------------------------------------- |
| `customer_id`               | `INTEGER` | The unique identifier for the customer.            |
| `full_name`                 | `STRING`  | The full name of the customer.                     |
| `address`                   | `STRING`  | The address of the customer.                       |
| `phone_number`              | `STRING`  | The contact phone number for the customer.         |
| `sales_date_month`          | `DATE`    | The first day of the month for which sales are aggregated.|
| `total_sales`               | `DECIMAL` | The total amount spent by the customer in that month.     |


### 2. Salesperson Monthly Performance
This data mart identifies the top-performing salesperson in each store every month.
* **Partitioned by**: `store_id`, `sales_person_id`, `sales_month`

| Column Name       | Data Type | Description                                                               |
| ----------------- | --------- | ------------------------------------------------------------------------- |
| `store_id`        | `INTEGER` | The unique identifier for the store.                                      |
| `sales_person_id` | `INTEGER` | The unique identifier for the salesperson.                                |
| `full_name`       | `STRING`  | Full name of the salesperson.                                             |
| `sales_month`     | `DATE`    | The first day of the month for which sales are aggregated.                |
| `total_sales`     | `DECIMAL` | The total revenue generated by this salesperson for the month.            |
| `incentive`       | `DECIMAL` | The calculated 1% incentive amount (non-zero only for the top performer). |

---

## Project Setup and Execution

This section outlines how to set up and run the ETL pipeline.

### Prerequisites
* Java 21 (for PySpark)
* Python 3.8+
* An AWS account with S3 access credentials configured.
* Access to the MySQL database.
* Spark configured locally

Clone the Repository: \
`git clone https://github.com/abhishektarun09/sales_incentive_de_project.git`

Create Virtual Environment: \
`python -m venv venv && source venv/bin/activate`

Install dependencies using: \
`python setup.py install`

Run: \
`python src\main\transformations\jobs\main.py`

## Contact

- Author: Abhishek Tarun
- Email: abhishek.tarun09@gmail.com 